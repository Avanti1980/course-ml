---
presentation:
  margin: 0
  center: false
  transition: "none"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 基本概念 上

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide vertical=true data-notes="" -->

##### 大纲

---

@import "../vega/outline.json" {as="vega" .top-2}

<!-- slide data-notes="" -->

##### 机器学习定义

---

机器学习之父<a href="https://en.wikipedia.org/wiki/Tom_M._Mitchell" target=_balnk>汤姆·米切尔</a>，1997 年出版《机器学习》

<div class="bottom-4"></div>

> A {==computer program==} is said to learn from {==experience E==} with respect to some class of {==tasks T==} and {==performance measure P==}, if its performance at tasks in T, as measured by P, improves with experience E.

<div class="bottom-6"></div>

> DeepSeek 的翻译: 若计算机程序在某类任务 T 中的表现 (以评估指标 P 衡量) 随经验 E 的积累而提升，则称该程序具备从经验 E 中学习的能力。

<p class="footnote book"> 汤姆·米切尔，Tom M. Mitchell，美国卡内基梅隆大学计算机学院院长，美国国家工程院、美国艺术与科学院院士，国际人工智能促进会 (AAAI) 前任主席，国际机器学习大会 (ICML) 创始人之一</p>

<!-- slide vertical=true data-notes="" -->

##### 机器学习定义

---

若计算机程序在某类任务 T 中的表现 (以评估指标 P 衡量) 随经验 E 的积累而提升，则称该程序具备从经验 E 中学习的能力。

四个关键元素

<ul>
    <li>程序：某个机器学习算法的具体实现</li>
    <li>经验：通常以<span class="blue">数据</span>的形式存在</li>
    <li>任务：各种类型，监督学习、半监督学习、无监督学习、……</li>
    <li>评估：不同类型的任务有不同的评估指标</li>
</ul>

<!-- slide data-notes="" -->

##### 数据

---

结构型数据：二维表格

<div class="threelines column4-border-right-solid head-highlight-1 tr-hover top-2">

| 花萼长度 | 花萼宽度 | 花瓣长度 | 花瓣宽度 |   类别标记   |
| :------: | :------: | :------: | :------: | :----------: |
|   5.1    |   3.5    |   1.4    |   0.2    |    山鸢尾    |
|   4.9    |   3.0    |   1.4    |   0.2    |    山鸢尾    |
|   7.0    |   3.2    |   4.7    |   1.4    |   杂色鸢尾   |
|   6.4    |   3.2    |   4.5    |   1.5    |   杂色鸢尾   |
|   6.2    |   3.4    |   5.4    |   2.3    | 维吉尼亚鸢尾 |
|   5.9    |   3.0    |   5.1    |   1.8    | 维吉尼亚鸢尾 |

</div>

- 行：样本 (example, sample) or 示例 (instance)
- 列：特征 (feature) or 属性 (attribute)，最后一列为类别标记 (label)

<p class="footnote book"> 该表格取自<a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" target=_blank>鸢尾花数据集</a>，由美国植物学家 <a href="https://en.wikipedia.org/wiki/Edgar_Anderson" target=_blank>Edgar Shannon Anderson</a> 收集，英国统计学家 <a href="https://en.wikipedia.org/wiki/Ronald_Fisher" target=_blank>Ronald Aylmer Fisher</a> 引入到统计分析中，共有 150 个样本、3 个类别，每类 50 个样本</p>

<!-- slide vertical=true data-notes="" -->

##### 鸢尾花数据集

---

sklearn 集成了该数据集

@import "../python/iris-info.py" {line_begin=0 line_end=107 .line-numbers .top-1 .left4 highlight=[]}

<!-- slide vertical=true data-notes="" -->

##### 鸢尾花数据分布可视化

---

将样本想象为欧几里得空间中的点

@import "../python/dt-iris-plot.svg" {.center .width75 .top1}

<!-- slide vertical=true data-notes="" -->

##### 数据

---

非结构型数据，不限于下面四种

<div class="multi_column left8">
    <div style="display:flex;flex-direction:column;margin-right:4%">
        <p class="center">图片</p>
        <img src="../img/data/img1.jpg" width=185px>
        <img src="../img/data/img2.jpg" width=185px>
    </div>
    <div style="display:flex;flex-direction:column;margin-right:4%">
        <p class="center">文本</p>
        <img src="../img/data/text1.jpg" width=185px>
        <img src="../img/data/text2.jpg" width=185px>
    </div>
    <div style="display:flex;flex-direction:column;margin-right:4%">
        <p class="center">语音</p>
        <img src="../img/data/audio1.jpg" width=185px>
        <img src="../img/data/audio2.jpg" width=185px>
    </div>
    <div style="display:flex;flex-direction:column;">
        <p class="center">棋盘</p>
        <img src="../img/data/chess1.jpg" width=185px>
        <img src="../img/data/chess2.jpg" width=185px>
    </div>
</div>

<!-- slide data-notes="" -->

##### 监督学习

---

所有样本都有标记

<div class="threelines column1-border-right-solid head-highlight-1 tr-hover top-2 bottom-2">

| 原始数据 |   样本/示例    | 属性/特征 | 类别标记 |
| :------: | :------------: | :-------: | :------: |
|  $o_1$   | $(\xv_1, y_1)$ |  $\xv_1$  |  $y_1$   |
|  $o_2$   | $(\xv_2, y_2)$ |  $\xv_2$  |  $y_2$   |
| $\vdots$ |    $\vdots$    | $\vdots$  | $\vdots$ |
|  $o_m$   | $(\xv_m, y_m)$ |  $\xv_m$  |  $y_m$   |

</div>

任务类型：

- 二分类 (binary classification)：$y \in \{ 1, -1 \}$或者$y \in \{ 1, 0 \}$
- 多分类 (multi-class classification)：$y \in [c] \triangleq \{ 1, 2, \ldots, c \}$
- 回归 (regression)：$y \in \Rbb$或连续集合
- 结构预测：$y$是向量、有序列表、序列、句法树、……

<!-- slide vertical=true data-notes="" -->

##### 二分类任务示例

---

乳腺癌预测：美国威斯康星大学医院 William H. Wolberg 博士及其合作者于 1995 年收集

一共 569 个样本，2 个类别，其中恶性 (212)、良性 (357)

特征从乳腺肿块细针穿刺的数字化图像计算而来，对每张图像中的细胞核计算 radius (半径)、texture (纹理)、perimeter (周长)、area (面积)、smoothness (平滑度)、compactness (紧凑度)、concavity (凹度)、concave points (凹点)、symmetry (对称性)、fractal dimension (分形维数) 等 10 个特征，然后取平均值、标准差、极值，共得到 30 个特征

<!-- slide vertical=true data-notes="" -->

##### 二分类任务示例

---

sklearn 集成了该数据集

@import "../python/cancer-info.py" {line_b e g in=0 line_end=119 .line-numbers .top-1 .left4 highlight=[]}

<!-- slide data-notes="" -->

##### 多分类任务示例

---

手写数字识别：美国国家标准与技术研究所 (National Institute of Standards and Technology, NIST) 收集，共 10 个类别

- <a href="https://en.wikipedia.org/wiki/MNIST_database" target=_blank>MNIST</a> (Modified NIST) 数据集收集了其中 70000 张图片，其中 60000 张训练、10000 张测试，每张图片为 28 x 28 像素
- <a href="https://archive.ics.uci.edu/datasets" target=_blank>UCI</a> 数据集收集了其中 5620 张图片，其中 3823 张训练、1797 张测试，每张图片为 8 x 8 像素，下图为测试集中的前 40 张图片

@import "../img/number.svg" {.center .top4 .width70 title=""}

<!-- slide vertical=true data-notes="" -->

##### 多分类任务示例

---

sklearn 中的 digits 数据集就是 UCI 数据集中的测试集

@import "../python/digit-info.py" {line_begin=1 line_end=48 .line-numbers .top-1 .left4 highlight=[]}

<!-- slide data-notes="" -->

##### 回归任务示例

---

糖尿病检测：美国国立糖尿病与消化与肾脏疾病研究所收集，一共 442 个样本，$y$是糖尿病人一年后病情进展的定量测量，每个样本包含 10 个特征：

- 年龄
- 性别
- 身体质量指数
- 平均血压
- 血清总胆固醇
- 低密度脂蛋白
- 高密度脂蛋白
- 总胆固醇 / 高密度脂蛋白
- 血清甘油三酯水平的对数
- 血糖水平

<!-- slide vertical=true data-notes="" -->

##### 回归任务示例

---

sklearn 集成了该数据集

@import "../python/diabetes-info.py" {line_begin=0 line_end=119 .line-numbers .top-1 .left4 highlight=[]}

<!-- slide vertical=true data-notes="" -->

##### 结构预测任务示例

---

$y$是向量

- 多分类，标量$y \in [c]$经独热编码 (one-hot encoding) 后变成长度为$c$的向量
- 多标记学习 (multi-label learning)，用于图片中的多物体识别任务

<div class="bottom2"></div>

<p class="fragment" data-fragment-index="1">$y$是有序列表，用于信息检索任务</p>

<p class="fragment" data-fragment-index="2">$y$是序列，用于机器翻译、问答系统等任务</p>

<p class="fragment" data-fragment-index="3">$y$是句法树，用于对自然语言的句法分析</p>

<img src="../img/tree.png" class="fragment top-15 width22 bottom-10" style="margin-right:6%;margin-left:auto" data-fragment-index="3">

<!-- slide data-notes="" -->

##### 半监督学习

---

只有部分样本有标记，常见于标记获许代价很高的场景，关键问题如何利用未标记样本的分布信息辅助对有标记样本的学习？

<div class="threelines column1-border-right-solid row4-border-top-dashed head-highlight-1 tr-hover top-2 bottom-2">

| 原始数据  |    样本/示例     |  属性/特征  | 类别标记 |
| :-------: | :--------------: | :---------: | :------: |
|   $o_1$   |  $(\xv_1, y_1)$  |   $\xv_1$   |  $y_1$   |
| $\vdots$  |     $\vdots$     |  $\vdots$   | $\vdots$ |
|   $o_l$   |  $(\xv_l, y_l)$  |   $\xv_m$   |  $y_l$   |
| $o_{l+1}$ | $(\xv_{l+1}, -)$ | $\xv_{l+1}$ |   $-$    |
| $\vdots$  |     $\vdots$     |  $\vdots$   | $\vdots$ |
| $o_{l+u}$ | $(\xv_{l+u}, -)$ | $\xv_{l+u}$ |   $-$    |

</div>

任务类型：

- 转导 (transductive) 学习：只需预测$\xv_{l+1}, \ldots, \xv_{l+u}$的标记
- 归纳 (inductive) 学习：必须有显式模型，能对未知样本进行预测

<!-- slide data-notes="" -->

##### 无监督学习

---

所有样本都没有标记

<div class="threelines column1-border-right-solid head-highlight-1 tr-hover top-2 bottom-2">

| 原始数据 |  样本/示例   | 属性/特征 | 类别标记 |
| :------: | :----------: | :-------: | :------: |
|  $o_1$   | $(\xv_1, -)$ |  $\xv_1$  |   $-$    |
|  $o_2$   | $(\xv_2, -)$ |  $\xv_2$  |   $-$    |
| $\vdots$ |   $\vdots$   | $\vdots$  | $\vdots$ |
|  $o_m$   | $(\xv_m, -)$ |  $\xv_m$  |   $-$    |

</div>

任务类型：

- 聚类 (clustering)：依相似度将数据分成若干个簇 (cluster)
- 降维 (dimension reduction)：为样本学习更具代表性的特征表示
- 密度估计 (density estimation)：估计数据所来自的概率分布$p(\xv)$

<!-- slide vertical=true data-notes="" -->

##### 聚类

---

最具代表性的是$k$-均值 ($k$-means) 算法，其中$k$是目标簇数，需事先指定 (算法的输入)

基本想法：设数据聚成$k$个簇$\Ccal_1, \ldots, \Ccal_k$，每个样本都恰属于某一个簇，每个样本到所在簇簇中心的距离小于与其它簇中心的距离

优化问题：$\min_{\muv_i} \sum_{i \in [k]} \sum_{\xv \in \Ccal_i} \| \xv - \muv_i \|_2^2, ~ \st ~ \muv_i = \sum_{\xv \in \Ccal_i} \xv / |\Ccal_i|.$

求解算法：随机初始化$k$个簇中心，重复以下两步直至收敛：1) 将每个样本分配到距其最近的簇中心；2) 更新每个簇的中心

<p class="footnote book"> $k$-均值这一叫法由 James MacQueen 于 1967 年首次使用，但其思想可追溯到 1957 年的 Hugo Steinhaus。上述求解算法最初在 1957 年由贝尔实验室的 Stuart Lloyd 作为一种脉冲码调制技术提出，但直到 1982 年才公开发表，最终结果是优化问题的局部最优解 (不是全局最优) 且受簇中心的初始化影响。1965 年，Edward W. Forgy 发表了本质上相同的方法，故该算法有时也称为 Lloyd–Forgy 方法。</p>

<!-- slide vertical=true data-notes="" -->

##### 聚类

---

从 6 个中心随机生成、各向同性、标准差为 1 的正态分布里各采样 2000 个样本，并指定聚成 4 个簇

@import "../python/clustering.svg" {.center .top2 .width60}

<!-- slide data-notes="" -->

##### 降维

---

<div class="top2"></div>

- 输入：$\Rbb^D$中的$m$个点，写成矩阵的形式$\Xv \in \Rbb^{m \times D}$
- 输出：$\Xv \Wv \in \Rbb^{m \times d}$，其中$\Wv = [\wv_1, \ldots, \wv_d] \in \Rbb^{D \times d}$为投影矩阵、$\wv_1, \ldots, \wv_d$为相互正交的单位向量、$d < D$

<div class="top4"></div>

$$
\begin{align*}
    \quad \Xv \in \Rbb^{m \times D} \xrightarrow[\text{降维}]{\Wv \in \Rbb^{D \times d}} \Xv \Wv \in \Rbb^{m \times d} \xrightarrow[\text{重构}]{\Wv^\top \in \Rbb^{d \times D}} \Xv \Wv \Wv^\top \in \Rbb^{m \times D}
\end{align*}
$$

主成分分析 (principal components analysis, PCA) 的目标为{==最小化重构误差==}

$$
\begin{align*}
    \quad \min_{\Wv} \| \Xv - \Xv \Wv \Wv^\top \|_F^2, \quad \st ~ \Wv^\top \Wv = \Iv.
\end{align*}
$$

<p class="footnote book"> PCA 由 Karl Pearson 于 1901 年提出，之后在 1930 年左右由 Harold Hotelling 独立发展并命名。在力学中叫主轴定理 (principal axis theorem)，在信号处理中叫做离散 K-L 转换 (discrete Karhunen–Loève transform, KLT)。</p>

<!-- slide vertical=true data-notes="" -->

##### 降维

---

输入为$\Rbb^2$中的 500 个点，服从正态分布 (等高线为一族椭圆)，$\Rbb^2 \mapsto \Rbb$的重构误差最小的投影方向 (成分) 是椭圆的长轴

@import "../python/pca-plot.svg" {.center .top2 .width70 title="Component 0 是重构误差最小的投影方向"}

<!-- slide data-notes="" -->

##### 密度估计

---

直方图估计：选定 bin 的起始点和宽度，统计样本落在每个 bin 中的个数，再归一化就是$p(\xv)$，因此直方图估计就是数数

直方图估计对 bin 的选取很敏感

@import "../python/density-estimation-1.svg" {.center .top2 .width90}

20 个样本，其中 6 个采样自$\Ncal(0,1)$，14 个采样自$\Ncal(0,5)$

<!-- slide vertical=true data-notes="" -->

##### 密度估计

---

假设某 bin 为区间$\Ical = [x-h, x+h]$，则点$x$处的概率估计为

$$
\begin{align*}
    \quad p(x) = \frac{1}{mh} \sum_{i \in [m]} \frac{1}{2} \Ibb \left( \left| \frac{x - x_i}{h} \right| \le 1 \right) = \frac{1}{mh} \sum_{i \in [m]} \kappa \left( \frac{x - x_i}{h} \right)
\end{align*}
$$

<div class="top-4"></div>

其中$\Ibb(\cdot)$是指示函数 (indicator function)、$\kappa(u) = \frac{1}{2} \Ibb(|u| \le 1)$

<div class="top2"></div>

直方图直接用区间中的样本数估计$p(x)$，换言之区间中的所有样本重要性是相同的，这样的估计很粗糙，不同$x_i$与$x$的距离不同，重要性理应不同，例如若希望$x_i$的重要性随着与$x$的距离增加而线性减小，这可通过修改$\kappa$来实现

$$
\begin{align*}
    \quad \kappa(u) = (1 - |u|) \Ibb(|u| \le 1) = \underbrace{\left( 1 - \left| \frac{x - x_i}{h} \right| \right)}_{\text{重要性权重}} \Ibb \left( \left| \frac{x - x_i}{h} \right| \le 1 \right)
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 密度估计

---

类似$\kappa(u) = (1 - |u|) \Ibb(|u| \le 1)$之类关于$0$中心对称的非负函数称为核函数 (kernel function)，对应的估计为核密度估计 (kernel density estimation, KDE)

<div class="threelines column1-border-right-solid head-highlight-1 tr-hover left4 righta tighttable">

|      核      |                          $\kappa(u)$                           |
| :----------: | :------------------------------------------------------------: |
|    tophat    |             $\frac{1}{2} \Ibb( \shu u \shu \le 1)$             |
|    linear    |          $(1 - \shu u \shu) \Ibb(\shu u \shu \le 1)$           |
| epanechnikov |        $\frac{3}{4} (1 - u^2) \Ibb(\shu u \shu \le 1)$         |
|    cosine    | $\frac{\pi}{4} \cos (\frac{\pi}{2} u) \Ibb(\shu u \shu \le 1)$ |
|   gaussian   |        $\frac{1}{\sqrt{2 \pi}} \exp (-\frac{1}{2} u^2)$        |
| exponential  |               $\frac{1}{2} \exp (- \shu u \shu)$               |

</div>

@import "../python/density-estimation-2.svg" {.top-40 .width56 .right4 .lefta}

<p class="footnote book"> sklearn 提供了上面 6 种核函数，此外$\frac{15}{16} (1 - u^2)^2 \Ibb(|u| \le 1)$、$\frac{35}{32} (1 - u^2)^3 \Ibb(|u| \le 1)$也是常用核函数。</p>

<!-- slide vertical=true data-notes="" -->

##### 密度估计

---

100 个样本，其中 30 个采样自$\Ncal(0,1)$，70 个采样自$\Ncal(0,5)$

@import "../python/density-estimation-3.svg" {.center .top3 .width90}

<!-- slide data-notes="" -->

##### 其他学习任务

---

强化学习 (reinforcement learning)：研究智能体 (agent) 在环境 (environment)中如何根据状态 (state) 采取行动 (action) 以最大化期望收益 (reward)

- 智能体：炒股机器人
- 环境：股票市场
- 状态：股票价格、手中筹码、剩余资金
- 行动：买/卖股票
- 收益：赚钱就是正收益，赔钱就是负收益

<div class="top2"></div>

迁移学习 (transfer learning)：假设在旧任务上已经学得一个模型，如何利用该模型辅助新任务的学习，任务间的相似度很关键

- 旧任务：C++，新任务：Python

<p class="footnote comments"> 受课时所限，本课程不会涉及这些机器学习任务，前者可单独作为一门课程，后者也至少需要 2-3 周的研讨班，此外还有主动学习 (active learning)、元学习 (meta learning) 等，不一而足。</p>

<!-- slide data-notes="" -->

##### 机器学习方法分类

---

米哈尔斯基 等《机器学习：一种人工智能途径》<br>_Machine Learning: An Artificial Intelligence Approach_, 1983

- 从样本中学习
- 在问题求解和规划中学习
- 通过观察和发现学习
- 从指令中学习

<div class="top2"></div>

费根鲍姆 等《人工智能手册》<br>_The Handbook of Artificial Intelligence_, 1983

- 机械学习，死记硬背式学习，信息存储检索
- 示教学习，类似于“从指令中学习”
- 类比学习，类似于“通过观察和发现学习”
- {==归纳学习==}，类似于“从样本中学习”，目前研究最多、应用最广

<!-- slide vertical=true data-notes="" -->

##### 机器学习方法分类

---

佩德罗·多明戈斯于 2015 年在《终极算法》中提出

- 符号学派：规则学习，决策树
- 连接学派：感知机，神经网络
- {--进化学派--}
- 统计学派：朴素贝叶斯，贝叶斯网
- 类推学派：k-近邻，支持向量机

<img src="../img/book/master.jpg" class="top-20 width22 bottom-10" style="margin-right:6%;margin-left:auto">

<p class="footnote book"> 佩德罗·多明戈斯，Pedro Domingos，华盛顿大学计算机科学与工程学院终身名誉教授，以研究能进行不确定推断的马尔可夫逻辑网而在机器学习领域闻名，2015 年出版 <i>The Master Algorithm</i> 一书，中文译作《终极算法》。</p>

<!-- slide data-notes="" -->

##### 约会问题

---

<div class="threelines column7-border-right-solid head-highlight-1 tr-hover row5-border-top-dashed">

| 次序 | 时间 | 方式 | 天气 | 课业 | 疫情 | 电视 | 约会 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  1   | 周六 | 吃饭 | 晴天 | 轻松 | 清零 | 精彩 |  是  |
|  6   | 周六 | 逛街 | 晴天 | 轻松 | 平缓 | 无聊 |  是  |
|  10  | 周六 | 学习 | 雨天 | 轻松 | 严峻 | 无聊 |  否  |
|  13  | 周六 | 逛街 | 晴天 | 适中 | 清零 | 精彩 |  否  |
|  14  | 周间 | 逛街 | 阴天 | 适中 | 清零 | 精彩 |  ?   |

</div>

<div class="top4"></div>

### 约还是不约，让{==机器学习==}来拯救你

<!-- slide vertical=true data-notes="" -->

##### 符号学派

---

<div class="threelines column7-border-right-solid head-highlight-1 tr-hover row5-border-top-dashed">

| 次序 | 时间 | 方式 | 天气 | 课业 | 疫情 | 电视 | 约会 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  1   | 周六 | 吃饭 | 晴天 | 轻松 | 清零 | 精彩 |  是  |
|  6   | 周六 | 逛街 | 晴天 | 轻松 | 平缓 | 无聊 |  是  |
|  10  | 周六 | 学习 | 雨天 | 轻松 | 严峻 | 无聊 |  否  |
|  13  | 周六 | 逛街 | 晴天 | 适中 | 清零 | 精彩 |  否  |
|  14  | 周间 | 逛街 | 阴天 | 适中 | 清零 | 精彩 |  ?   |

</div>

用 {==if-then==} 形式的{==合取规则==}尽可能地概括正样本

<p class="left10">$\text{是} \longleftarrow (\text{天气} = \text{晴天}) \wedge (\text{课业} = \text{轻松})$</p>

<div class="top2"></div>

- 可解释强，人类可以秒懂
- 将规则集合组织成树的形式即为{==决策树==} (decision tree)

<!-- slide vertical=true data-notes="" -->

##### 连接学派

---

<div class="threelines column7-border-right-solid head-highlight-1 tr-hover row5-border-top-dashed">

| 次序 | 时间 | 方式 | 天气 | 课业 | 疫情 | 电视 | 约会 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  1   | 周六 | 吃饭 | 晴天 | 轻松 | 清零 | 精彩 |  是  |
|  6   | 周六 | 逛街 | 晴天 | 轻松 | 平缓 | 无聊 |  是  |
|  10  | 周六 | 学习 | 雨天 | 轻松 | 严峻 | 无聊 |  否  |
|  13  | 周六 | 逛街 | 晴天 | 适中 | 清零 | 精彩 |  否  |
|  14  | 周间 | 逛街 | 阴天 | 适中 | 清零 | 精彩 |  ?   |

</div>

用{==感知机==} (perceptron) 拟合数据

<p class="left10">$\{1, -1\} \longleftarrow \sign(w_0 + w_1 \cdot \text{次序} + \cdots + w_7 \cdot \text{电视})$</p>

<div class="top2"></div>

- 数据中的知识体现为感知机的参数$w_0, w_1, \ldots, w_7$
- 将大量感知机并行串联就是{==神经网络==} (neural network)

<!-- slide vertical=true data-notes="" -->

##### 统计学派

---

<div class="threelines column7-border-right-solid head-highlight-1 tr-hover row5-border-top-dashed">

| 次序 | 时间 | 方式 | 天气 | 课业 | 疫情 | 电视 | 约会 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  1   | 周六 | 吃饭 | 晴天 | 轻松 | 清零 | 精彩 |  是  |
|  6   | 周六 | 逛街 | 晴天 | 轻松 | 平缓 | 无聊 |  是  |
|  10  | 周六 | 学习 | 雨天 | 轻松 | 严峻 | 无聊 |  否  |
|  13  | 周六 | 逛街 | 晴天 | 适中 | 清零 | 精彩 |  否  |
|  14  | 周间 | 逛街 | 阴天 | 适中 | 清零 | 精彩 |  ?   |

</div>

利用{==贝叶斯公式==}求{==后验==} (posterior)

<p class="left10">$p (\text{约会}|\text{次序},\text{时间},\ldots,\text{电视}) = \frac{p(\text{次序},\text{时间},\ldots,\text{电视}|\text{约会}) ~ p(\text{约会}) \quad \quad \quad ~~~~~}{p(\text{次序},\text{时间},\ldots,\text{电视}) \qquad \quad}$</p>

<div class="top2"></div>

- $p (\text{约会})$为{==先验==} (prior)，反映在没有任何信息的情况下对约会的信念
- 数据通过{==似然==} (likelihood) $ p (\text{次序},\ldots,\text{电视}|\text{约会})$调整对约会的信念

<!-- slide vertical=true data-notes="" -->

##### 类推学派

---

<div class="threelines column7-border-right-solid head-highlight-1 tr-hover row5-border-top-dashed">

| 次序 | 时间 | 方式 | 天气 | 课业 | 疫情 | 电视 | 约会 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  1   | 周六 | 吃饭 | 晴天 | 轻松 | 清零 | 精彩 |  是  |
|  6   | 周六 | 逛街 | 晴天 | 轻松 | 平缓 | 无聊 |  是  |
|  10  | 周六 | 学习 | 雨天 | 轻松 | 严峻 | 无聊 |  否  |
|  13  | 周六 | 逛街 | 晴天 | 适中 | 清零 | 精彩 |  否  |
|  14  | 周间 | 逛街 | 阴天 | 适中 | 清零 | 精彩 |  ?   |

</div>

引入{==相似度==}函数$s(\cdot, \cdot)$和样本{==权重==}$\alpha$

<p class="left10">$\{1, -1\} \longleftarrow \sign(\alpha_1 \cdot s(\xv_1, \xv_5) \cdot y_1 + \cdots + \alpha_4 \cdot s(\xv_4, \xv_5) \cdot y_4)$</p>

<div class="top2"></div>

- {==k-近邻==}：$s(\cdot, \cdot)$为欧氏距离，最近的$k$个样本权重为$1/s$，其余为$0$
- {==支持向量机==}：$s(\cdot, \cdot)$为核函数，权重为对偶问题的拉格朗日乘子变量

<!-- slide data-notes="" -->

##### 百家争鸣

---

Q：哪个算法更好？

<p class="fragment bottom6" data-fragment-index="1">A：不存在最强的算法，否则机器学习课把最强算法一讲就可以结课了</p>

<p class="fragment" data-fragment-index="2">假设$\xv \in \{ 0,1 \}^2$、$y \in \{0,1\}$</p>

<div class="threelines column2-border-right-solid column3-border-right-solid row2-column1-border1-bottom-dashed row2-column2-border1-bottom-dashed row2-column3-border1-bottom-dashed head-highlight-1 tr-hover fragment" data-fragment-index="2">

| $x_1$ | $x_2$ | $y$ |  薛吒  |  薛跋  |  薛深  |
| :---: | :---: | :-: | :----: | :----: | :----: |
|   0   |   0   |  0  | 抛硬币 | 决策树 | 感知机 |
|   1   |   1   |  1  |   ^    |   ^    |   ^    |
|   1   |   0   |  ?  |   ^    |   ^    |   ^    |
|   0   |   1   |  ?  |   ^    |   ^    |   ^    |

</div>

<!-- slide vertical=true data-notes="" -->

##### 百家争鸣

---

<div class="threelines column2-border-right-solid column3-border-right-solid row3-border-top-dashed head-highlight-1 tr-hover top-1 bottom-3">

| $x_1$ | $x_2$ | $y$ |             薛吒             |            薛跋            |                     薛深                     |
| :---: | :---: | :-: | :--------------------------: | :------------------------: | :------------------------------------------: |
|   0   |   0   |  0  | $ \sgn(\text{rand}() - 0.5)$ | $ x_1 = 1 \wedge x_2 = 1 $ | $ \sgn(0.7 \cdot x_1 + 0.3 \cdot x_2 - 0.5)$ |
|   1   |   1   |  1  |              ^               |             ^              |                      ^                       |
|   1   |   0   |  ?  |              1               |             0              |                      1                       |
|   0   |   1   |  ?  |              1               |             0              |                      0                       |

</div>

<div class="threelines column1-border-right-solid column2-border-right-solid head-highlight-1 tr-hover row5-border-top-dashed fragment bottom-2" data-fragment-index="1">

|    真实模型    | $\yv$ |    薛吒    |    薛跋    |    薛深    |
| :------------: | :---: | :--------: | :--------: | :--------: |
| $x_1 \cup x_2$ | 1、1  | 全对，完胜 | 全错，完败 | 对一，错一 |
| $x_1 \cap x_2$ | 0、0  | 全错，完败 | 全对，完胜 | 对一，错一 |
|     $x_1$      | 1、0  | 对一，错一 | 对一，错一 | 全对，完胜 |
|     $x_2$      | 0、1  | 对一，错一 | 对一，错一 | 全错，完败 |

</div>

<p class="fragment" data-fragment-index="2">若四个真实模型出现概率相同，则所有算法期望表现相同</p>

<p class="footnote book"> $\sgn(\text{正数}) = 1$、$\sgn(\text{负数}) = 0$、$\sgn(\cdot) = (\sign(\cdot) + 1)/2$</p>

<!-- slide data-notes="" -->

##### 没有免费的午餐

---


前页的结果并非偶然，在对真实模型一无所知 (只能假设等概率出现) 的情况下，所有算法的期望错误率均为$1/2$，与随便猜等同，这称为没有免费的午餐 (no free lunch, NFL) 定理

设数据集$D \subseteq (\Xcal \times \{0,1\})^m$，其中$\Xcal$是{==离散==}的，$p(f \mid A, D)$为算法$A$基于$D$产生模型$f$的概率

模型集合$\Gcal = \{ \Xcal \mapsto \{0,1\} \}$，易知$|\Gcal| = 2^{|\Xcal|}$

给定$g \in \Gcal$为真实模型，则算法$A$的期望预测错误率为

$$
\begin{align*}
    \quad E (A \mid D, g) = \Ebb_\xv \int_f \Ibb(f(\xv) \ne g(\xv)) \cdot p(f \mid A, D) ~ \diff f
\end{align*}
$$

<p class="footnote book"> 模型集合$\Gcal$在机器学习理论中称为假设空间 (hypothesis space)。</p>

<!-- slide vertical=true data-notes="" -->

##### 没有免费的午餐

---

假设$\Gcal$中模型为真实模型的概率均为$1 / |\Gcal|$，则算法$A$的期望预测错误率为

$$
\begin{align*}
    \quad \sum_g \frac{E (A \mid D, g)}{|\Gcal|} & = \sum_g \frac{1}{|\Gcal|} \Ebb_\xv \int_f \Ibb(f(\xv) \ne g(\xv)) \cdot p(f \mid A, D) ~ \diff f \\
    & = \Ebb_\xv \int_f p(f \mid A, D) \frac{1}{|\Gcal|} \underbrace{\sum_g \Ibb(f(\xv) \ne g(\xv))}_{|\Gcal| / 2} ~ \diff f \\
    & = \Ebb_\xv \int_f \frac{p(f \mid A, D)}{2} ~ \diff f = \Ebb_\xv \frac{1}{2} = \frac{1}{2}
\end{align*}
$$

<div class="top-4"></div>

其中第二行是因为$\Xcal$是离散的，对任意$\xv \in \Xcal$，$\Gcal$中的$2^{|\Xcal|}$个模型恰有一半预测$\xv$为正、一半预测$\xv$为负

<!-- slide vertical=true data-notes="" -->

##### 没有免费的午餐

---

NFL 定理的启示：脱离具体的任务空谈什么算法好没有意义！

开汽车 vs. 骑电瓶车

- 若任务是从华科去黄鹤楼，那算法“开汽车”好
- 若任务是下课后从西十二去西一食堂，那算法“骑电瓶车”好

<div class="bottom2"></div>

<p class="fragment">NFL 定理假设了$g$的等可能性，但根据已有数据其实可以确定假设空间中部分模型为真实模型的可能性较高、而另一些则较低，因此学习算法自身的{==偏倚==} (bias) 应与任务 (数据) 相匹配</p>

<!-- slide data-notes="" -->

##### 评估 回归

---

给定模型$f$、数据集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$

- 均方损失：$\frac{1}{m} \sum_{i \in [m]} (f(\xv_i) - y_i)^2$
- 均方根损失：$\sqrt{\frac{1}{m} \sum_{i \in [m]} (f(\xv_i) - y_i)^2}$
- 平均绝对损失：$\frac{1}{m} \sum_{i \in [m]} |f(\xv_i) - y_i|$
- Huber 损失：$\frac{1}{m} \sum_{i \in [m]} \begin{cases} (f(\xv_i) - y_i)^2, & |f(\xv_i) - y_i| \le \delta \\ 2 \delta (|f(\xv_i) - y_i| - \delta/2), & |f(\xv_i) - y_i| > \delta \end{cases}$
- 支持向量损失：$\frac{1}{m} \sum_{i \in [m]} \begin{cases} 0, & |f(\xv_i) - y_i| \le \delta \\ |f(\xv_i) - y_i| - \delta, & |f(\xv_i) - y_i| > \delta \end{cases}$

@import "../python/model-evaluation-mse.py" {line_end=10 .line-numbers .top2 .left4 highlight=[7,10]}

<!-- slide vertical=true data-notes="" -->

##### 评估 二分类

---

给定模型$f$、数据集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，$y_i \in \{1,-1\}$

- 0-1 损失：$\frac{1}{m} \sum_{i \in [m]} \Ibb (y_i f(\xv_i) < 0)$，即错误率 (error rate)
- 对数损失：$\frac{1}{m} \sum_{i \in [m]} \ln (1 + \exp( - y_i f(\xv_i)))$
- 指数损失：$\frac{1}{m} \sum_{i \in [m]} \exp (- y_i f(\xv_i))$
- 合页 (hinge) 损失：$\frac{1}{m} \sum_{i \in [m]} \max \{ 0, 1 - y_i f(\xv_i) \}$

@import "../python/model-evaluation-acc.py" {line_end=10 .line-numbers .top2 .left4 highlight=[7,10]}

<!-- slide vertical=true data-notes="" -->

##### 查准率 查全率 <span style="font-weight:900;">F1</span>

---

二分类结果的{==混淆矩阵==} (confusion matrix)

<div class="threelines row4-border-top-solid column1-border-right-solid column1-bold tr-hover top-2 bottom-2 left-20">

|             |  预测 正样本   |  预测 负样本   |
| :---------: | :------------: | :------------: |
| 真实 正样本 | $\TP$ (真正例) | $\FN$ (假反例) |
| 真实 负样本 | $\FP$ (假正例) | $\TN$ (真反例) |

</div>

{==查准率==} (precision)：预测的约会中有多少比例真的约会了

{==查全率==} (recall)：所有的约会中有多少比例被预测出来了

<div class="top3"></div>

$$
\begin{align*}
    & \quad \mathrm{precision} = \frac{\TP}{\TP + \FP}, \quad \mathrm{recall} = \frac{\TP}{\TP + \FN} \\[4pt]
    & \quad \mathrm{F1} = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}} = \frac{2 \cdot \TP}{\text{样本总数} + \TP - \TN \quad}
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 查准率 查全率 <span style="font-weight:900;">F1</span>

---

@import "../python/model-evaluation-f1.py" {line_end=23 .line-numbers .top-1 .left4 highlight=[9-10,14,17,20,23]}

<!-- slide vertical=true data-notes="" -->

##### 评估 多分类 混淆矩阵

---

@import "../python/multi-classif-confusion-matrix.svg" {.center .top2 .width65 title="混淆矩阵统计最终的预测结果，第 i 行第 j 列的值就是第 i 类样本被预测为第 j 类的样本个数"}

<!-- slide data-menu-title="sklearn中的性能度量" data-background-iframe="https://scikit-learn.org/stable/modules/model_evaluation.html" vertical=true data-background-interactive data-notes="" -->

<!-- slide data-notes="" -->

##### 模型选择

---

终极目标：在{==未知数据==}上表现好，即{==泛化==} (generalization) 好

样本空间$\Xcal \subseteq \Rbb^d$，标记空间$\Ycal$，$\Xcal \times \Ycal$上的{==未知==}概率分布$\Dcal$

给定模型$f$，{==训练==}数据集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，其中$(\xv_i, y_i) \overset{\mathrm{iid}}{\sim} \Dcal$

几点说明：

- 数据集细分为训练集、测试集，训练集 (training set) 用来学习模型
- 测试集 (test set) 用来评估模型，在训练时不可见
- 训练集和测试集中的样本都{==独立同分布==} (iid) 地来自分布$\Dcal$，若无独立同分布假设，无法保证学习效果
- 分布$\Dcal$定义在$\Xcal \times \Ycal$上，即允许同一样本有多种标记，标记有随机性
- 若$\Dcal$只定义在$\Xcal$上，样本标记由某未知函数给出，则为确定性情形

<!-- slide vertical=true data-notes="" -->

##### 模型选择

---

回归：{==经验==} (empirical) 均方误差、{==泛化==}均方误差分别为

$$
\begin{align*}
    \quad E_D (f) & = \frac{1}{m} \sum_{i \in [m]} (f(\xv_i) - y_i)^2 \\[4pt]
    \quad E_{\Dcal} (f) & = \Ebb_{(\xv,y) \sim \Dcal} [(f(\xv) - y)^2] = \Ebb_{D \sim \Dcal^m} [E_D (f)]
\end{align*}
$$

分类：{==经验==}错误率、{==泛化==}错误率分别为

$$
\begin{align*}
    \quad E_D (f) & = \frac{1}{m} \sum_{i \in [m]} \Ibb (f(\xv_i) \ne y_i) \\[4pt]
    \quad E_{\Dcal} (f) & = \Ebb_{(\xv,y) \sim \Dcal} [\Ibb(f(\xv) \ne y)] = \Ebb_{D \sim \Dcal^m} [E_D (f)]
\end{align*}
$$

在不致混淆的情况下，可统称为{==经验风险==}和{==泛化风险==} (risk)

<!-- slide vertical=true data-notes="" -->

##### 欠拟合 过拟合

---

数据分布：$p(x) = \Ucal[0,1]$，$y = \cos (3 \pi x  / 2) + \Ncal(0, 1) / 10$

学习算法：{==$n$阶多项式回归==}

$$
\begin{align*}
    \min_{w_j} ~ F (w_j) = \frac{1}{2} \sum_{i \in [m]} \left( \sum_{j=0}^n w_j x_i^j - y_i \right)^2
\end{align*}
$$

<div class="top-3"></div>

其中$w_0, w_1, \ldots, w_n$为待求参数

目标函数$F$关于$w_j$的导数为

$$
\begin{align*}
    \nabla_{w_j} F = \sum_{i \in [m]} \left( \sum_{j=0}^n w_j x_i^j - y_i \right) x_i^j
\end{align*}
$$

@import "../python/overfitting-sample.svg" {.lefta .right8 .width40 .top-45per title="训练集"}

<!-- slide vertical=true data-notes="" -->

##### 欠拟合 过拟合

---

@import "../python/overfitting.svg" {.center .width90 .bottom2 title="过拟合"}

左图：1 阶多项式{==欠拟合==} (underfitting)，经验均方误差很大

<div class="top-4"></div>

中图：4 阶多项式拟合地最好，最贴近真实模型 (groundtruth)

<div class="top-4"></div>

右图：30 阶多项式{==过拟合==} (overfitting)，经验均方误差很小

<div class="top-2"></div>

我的启示 选对模型 (归纳偏好) 至关重要！如何选？

<!-- slide vertical=true data-notes="" -->

##### 模型选择 验证

---

我的启示 选对模型 (归纳偏好) 至关重要！如何选？

事先确定一组候选模型集合$\{ f_1, f_2, \ldots, f_n \}$，从中挑选最好的

从训练集中随机选择一部分样本作为{==验证集==} (validation set)

- 在剩余的训练集上依次训练$f_1, f_2, \ldots, f_n$
- 在验证集上依次评估$f_1, f_2, \ldots, f_n$

<div class="top2"></div>

{==交叉验证==} (cross validation)：将训练集平均分为$n$份，第$i$轮

- 在其中的第$[n] \setminus \{ i \}$份上依次训练$f_1, f_2, \ldots, f_n$
- 在第$i$份上依次评估$f_1, f_2, \ldots, f_n$

遍历$i \in [n]$取平均作为$f_1, f_2, \ldots, f_n$的性能，从中挑选最好的

<!-- slide data-notes="" -->

##### 偏差方差分解

---

以回归问题为例，对任意样本$(\xv,y) \sim \Dcal$，均方误差可分解为

<div class="top0"></div>

$$
\begin{align*}
    \quad (f (\xv) & - y)^2 = (f (\xv) - \Ebb [y|\xv] + \Ebb [y|\xv] - y)^2 \\
    & = (f (\xv) - \Ebb [y|\xv])^2 + (\Ebb [y|\xv] - y)^2 + 2 (f (\xv) - \Ebb [y|\xv]) (\Ebb [y|\xv] - y)
\end{align*}
$$

<div class="top-3"></div>

其中{==条件期望$\Ebb [y|\xv]$与$y$无关==}，对交叉项有

$$
\begin{align*}
    \quad \Ebb_{(\xv,y)} & [(f (\xv) - \Ebb [y|\xv]) (\Ebb [y|\xv] - y) ] \\
    & = \iint (f (\xv) - \Ebb [y|\xv]) (\Ebb [y|\xv] - y) p(\xv, y) \diff \xv \diff y \\
    & = \int (f (\xv) - \Ebb [y|\xv]) \left( \int (\Ebb [y|\xv] - y) p(\xv, y) \diff y \right) \diff \xv \\
    & = \int (f (\xv) - \Ebb [y|\xv]) \underbrace{ ( \Ebb [y|\xv] p(\xv) - p(\xv) \overbrace{ \class{yellow}{\int y \cdot p(y|\xv) \diff y}}^{=~\Ebb [y|\xv]} )}_{=~0} \diff \xv = 0
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 偏差方差分解

---

$$
\begin{align*}
    \quad \Ebb_{(\xv,y)} [(f (\xv) - y)^2] & = \Ebb_{(\xv,y)} [(\overbrace{f (\xv) - \Ebb [y|\xv]}^{\text{与}y\text{无关}})^2] + \overbrace{\Ebb_{(\xv,y)} [(\Ebb [y|\xv] - y)^2]}^{\text{噪声}} \\
    & = \Ebb_{\xv} [(f (\xv) - \Ebb [y|\xv])^2] + \text{噪声}
\end{align*}
$$

<div class="top-3"></div>

第二项{==噪声==}来自$y$的随机性，问题固有，{==与模型$f$的选择无关==}

根据第一项，使得泛化均方误差最小的$f^\star (\xv) = \Ebb [y|\xv]$

- 由于我们不知道$p(y|\xv)$，因此没法精确计算$\Ebb [y|\xv]$
- 如果数据足够多，也可以通过蒙特卡洛模拟得到近似准确的$\Ebb [y|\xv]$
- 但通常我们只有大小为$m$的数据集$D$
- 不同的$D$上训练得到不同的$f_D$，不同的$f_D$对$\xv$有不同的预测

<div class="top2"></div>

数据集$D$的随机性也必须考虑进来，注意噪声与$D$无关，故

$$
\begin{align*}
    \quad E = \Ebb_D \Ebb_{(\xv,y)} [(f_D (\xv) & - y)^2] = \Ebb_{\xv} \Ebb_D [(f_D (\xv) - \Ebb [y|\xv])^2] + \text{噪声}
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 偏差方差分解

---

泛化均方误差$E = \Ebb_{\xv} \Ebb_D [(f_D (\xv) - \Ebb [y|\xv])^2] + \text{噪声}$

引入$\xv$的{==期望预测==}$\Ebb_D [f_D (\xv)]$，易知有分解

<div class="top1"></div>

$$
\begin{align*}
    \quad (f_D (\xv) - \Ebb [y|\xv])^2 & = (f_D (\xv) - \Ebb_D [f_D (\xv)] + \Ebb_D [f_D (\xv)] - \Ebb [y|\xv])^2 \\
    & = (f_D (\xv) - \Ebb_D [f_D (\xv)])^2 + (\Ebb_D [f_D (\xv)] - \Ebb [y|\xv])^2 \\
    & \qquad + 2 (f_D (\xv) - \Ebb_D [f_D (\xv)]) (\Ebb_D [f_D (\xv)] - \Ebb [y|\xv])
\end{align*}
$$

{==注意$\Ebb_D [f_D (\xv)]$与$D$无关==}，对交叉项有

$$
\begin{align*}
    \quad \Ebb_D [(f_D (\xv) - \Ebb_D [& f_D (\xv)]) (\overbrace{\Ebb_D [f_D (\xv)] - \Ebb [y|\xv]}^{\text{与}D\text{无关}})] \\
    & = (\Ebb_D [f_D (\xv)] - \Ebb [y|\xv]) \underbrace{\Ebb_D [f_D (\xv) - \Ebb_D [f_D (\xv)]]}_{=~0} = 0
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 偏差方差分解

---

$$
\begin{align*}
    E & = \Ebb_{\xv} \Ebb_D [(f_D (\xv) - \Ebb_D [f_D (\xv)])^2] + \Ebb_{\xv} \Ebb_D [(\overbrace{\Ebb_D [f_D (\xv)] - \Ebb [y|\xv]}^{\text{与}D\text{无关}})^2] + \text{噪声} \\
    & = \underbrace{\Ebb_{\xv} \Ebb_D [(f_D (\xv) - \Ebb_D [f_D (\xv)])^2]}_{\text{方差}} + \underbrace{\Ebb_{\xv} [(\Ebb_D [f_D (\xv)] - \Ebb [y|\xv])^2]}_{\text{偏差 }^2} + \text{噪声}
\end{align*}
$$

<div class="top-3"></div>

综上，泛化均方误差可分解为$E = \text{偏差}^2 + \text{方差} + \text{噪声}$

- $\text{偏差}^2 = \Ebb_{\xv} [(\Ebb_D [f_D (\xv)] - \Ebb [y|\xv])^2]$，期望预测与最优模型预测的差别，体现{==学习算法的拟合能力==}，越小拟合能力越强
- $\text{方差} = \Ebb_{\xv} \Ebb_D [(f_D (\xv) - \Ebb_D [f_D (\xv)])^2]$，$D$上模型的预测与期望预测的差别，体现{==学习算法对数据集扰动的敏感度==}，越小越不敏感
- $\text{噪声} = \Ebb_{(\xv,y)} [(\Ebb [y|\xv] - y)^2]$，问题固有，无法优化

<div class="top2"></div>

我的启示 我们要选择{==低偏差==}同时{==低方差==}的模型！

<!-- slide data-notes="" -->

##### 偏差方差分解

---

@import "../python/bias-var-dec.svg" {.center .width82 title="随机生成了 5 个数据集：1 阶多项式高偏差、低方差；30 阶多项式低偏差、高方差；4 阶多项式低偏差、低偏差，是最理想的模型"}

<!-- slide vertical=true data-notes="" -->

##### 偏差方差窘境

---

偏差、方差往往是两难选择，即便对于单模型亦存在

- 训练不足时，模型还很糙，拟合能力不强，偏差占主导
- 训练程度加深后，模型开始捕捉数据细节，方差占主导

@import "../tikz/bias-var-dec.svg" {.center .top5 .width45 title="很多学习算法都可控制训练程度，例如决策树可控制层数，神经网络可控制训练轮数，集成学习方法可控制基学习器个数"}
