---
presentation:
  margin: 0
  center: false
  transition: "convex"
  enableSpeakerNotes: true
  slideNumber: "c/t"
  navigationMode: "linear"
---

@import "../css/font-awesome-4.7.0/css/font-awesome.css"
@import "../css/theme/solarized.css"
@import "../css/logo.css"
@import "../css/font.css"
@import "../css/color.css"
@import "../css/margin.css"
@import "../css/table.css"
@import "../css/main.css"
@import "../plugin/zoom/zoom.js"
@import "../plugin/customcontrols/plugin.js"
@import "../plugin/customcontrols/style.css"
@import "../plugin/chalkboard/plugin.js"
@import "../plugin/chalkboard/style.css"
@import "../plugin/menu/menu.js"
@import "../js/anychart/anychart-core.min.js"
@import "../js/anychart/anychart-venn.min.js"
@import "../js/anychart/pastel.min.js"
@import "../js/anychart/venn-ml.js"

<!-- slide data-notes="" -->

<div class="bottom20"></div>

# 机器学习

<hr class="width50 center">

## 贝叶斯概率

<div class="bottom8"></div>

### 计算机学院 &nbsp;&nbsp; 张腾

#### _tengzhang@hust.edu.cn_

<!-- slide data-notes="" -->

##### 预测张老师科研水平

---

- 华科有$80\%$科研水平高的老师
- 讲课水平和科研水平匹配的概率是$90\%$
- 张老师的机器学习课讲得有点次
- 张老师科研水平高的概率是多少？

两个随机变量：$\Theta$为张老师科研水平高，$X$为张老师讲课水平次

根据贝叶斯公式

$$
\begin{align*}
    \quad \Pbb(\Theta|X) & = \frac{\Pbb(X|\Theta) \Pbb(\Theta)}{\Pbb(X)} = \frac{\Pbb(X|\Theta) \Pbb(\Theta)}{\Pbb(X|\Theta) \Pbb(\Theta) + \Pbb(X|\neg\Theta) \Pbb(\neg\Theta)} \\
    & = \frac{0.1 \times 0.8}{0.1 \times 0.8 + 0.9 \times 0.2} \approx 0.308
\end{align*}
$$

- 如果之前没上过张老师的课，只能根据经验盲猜$80\%$
- 在观测到张老师讲课次这一事实后，概率修正为$30.8\%$

<!-- slide vertical=true data-notes="注意在这个例子中 我们已经在用信念来表示概率了" -->

##### 贝叶斯公式的理解

---

$$
\begin{align*}
    \quad \underbrace{\Pbb(\Theta|X)}_{\text{后验}} & = \frac{\overbrace{\Pbb(X|\Theta)}^{\text{似然}} \overbrace{\Pbb(\Theta)}^{\text{先验}}}{\underbrace{\Pbb(X)}_{\text{证据}}} = \Pbb(X|\Theta) \Pbb(\Theta) \Big/ \int \Pbb(X|\Theta) \Pbb(\Theta) \diff \Theta
\end{align*}
$$

- 先验：对张老师科研水平的初始{==信念==}
- 似然：{==可观测==}的讲课水平与{==不可观测==}的科研水平匹配的可能性
- 证据：讲课水平可观测，因此叫证据 (evidence)，也有人译作事实
- 后验：在得到观测数据$X$后，对初始信念的修正

<div class="top2"></div>

概率是用来刻画不确定性的工具

- 频率主义：概率是{==独立重复试验==}中随机事件发生{==频率==}的极限
- 贝叶斯主义：概率是观测者对事件为真的{==主观信念==}

<div class="top1"></div>

我的批注 在这个例子里，我们已经在用贝叶斯主义的说法了

<!-- slide data-notes="" -->

##### 频率主义

---

{==不确定性==}来自事件，因为事件的结果是随机的

- {==概率==}是{==独立重复试验==}中随机事件发生{==频率==}的极限
- 随机事件有一些{==固有的未知参数==}，影响观测数据的生成
- 有了观测数据后，可通过{==极大似然==}估计参数
- 仅凭观测数据进行估计产生的误差通过{==置信区间==}来刻画

<div class="top2"></div>

以抛硬币为例

- 未知参数$\theta = \Pbb(\text{正面})$，是硬币固有的
- 观测数据$X$：$t$次抛掷中有$k$次正面
- 似然为二项式分布$\Pbb(X | \theta) = \binom{t}{k} \theta^k (1 - \theta)^{t-k}$
- 假设某次观测$X$是抛了$10$次全为正面，根据极大似然可知$\theta^{\text{ML}} = 1$
- 预测：第$11$次抛掷$100\%$是正面

<div class="top2"></div>

缺点：无法处理一次性事件

<!-- slide data-notes="" -->

##### 贝叶斯主义

---

{==不确定性==}来自观测者，因为{==观测者的信息不完全==}

以抛硬币为例

- $\theta = \Pbb(\text{正面})$不再是一个确定的数，而是$[0,1]$上的一个随机变量
- $\theta$的先验是观测者在抛掷前对抛出正面的初始信念
- $\theta$的后验是观测者得到数据 (信息) 后对先验的修正

<div class="top2"></div>

优点：

- 可以讨论一次性事件的概率了
- 先验为引入{==领域知识==}提供了途径
- 方便动态地处理数据，上一时刻的后验作为下一时刻的先验

<div class="top2"></div>

缺点：概率依赖于观测者，是观测者的主观信念，唯心主义？

<!-- slide vertical=true data-notes="" -->

##### 贝叶斯主义

---

数据$X$：$t$次抛掷中有$k$次正面，似然是{==二项式分布==}

$$
\begin{align*}
    \quad \Pbb(X | \theta) = \binom{t}{k} \theta^k (1 - \theta)^{t-k}
\end{align*}
$$

<div class="top-2"></div>

不妨设$\theta$的先验是参数为$(\alpha,\beta)$的{==贝塔分布==}：

$$
\begin{align*}
    \quad \Pbb(\theta) = \BetaDist(\theta|\alpha,\beta) = \frac{\theta^{\alpha - 1} (1-\theta)^{\beta - 1}}{\int_0^1 \theta^{\alpha - 1} (1-\theta)^{\beta - 1} \diff \theta} = \frac{\theta^{\alpha - 1} (1-\theta)^{\beta - 1}}{\BetaFunc(\alpha,\beta)}
\end{align*}
$$

<div class="top-4"></div>

其中$\BetaFunc(\alpha,\beta) = \int_0^1 \theta^{\alpha - 1} (1-\theta)^{\beta - 1} \diff \theta$是第一类欧拉积分，归一化用

根据贝叶斯公式

$$
\begin{align*}
    \quad \Pbb(\theta|X) = \frac{\Pbb(\theta) \Pbb(X|\theta)}{\Pbb(X)} = \Pbb(\theta) \Pbb(X|\theta) \Big/ \int_0^1 \Pbb(\theta) \Pbb(X|\theta) \diff \theta
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 共轭先验

---

证据和后验分别为

$$
\begin{align*}
    \quad \Pbb(X) & = \int_0^1 \Pbb(\theta) \Pbb(X|\theta) \diff \theta = \binom{t}{k} \frac{1}{\BetaFunc(\alpha,\beta)} \int_0^1  \theta^{\alpha + k - 1} (1 - \theta)^{\beta + t-k-1} \diff \theta \\
    & = \binom{t}{k} \frac{\BetaFunc(\alpha+k,\beta+t-k)}{\BetaFunc(\alpha,\beta)} \\[4pt]
    \Pbb(\theta|X) & = \frac{\Pbb(\theta) \Pbb(X|\theta)}{\Pbb(X)} = \frac{\theta^{\alpha + k - 1} (1-\theta)^{\beta + t - k - 1}}{\BetaFunc(\alpha+k,\beta+t-k)} = \BetaDist(\theta|\alpha+k,\beta+t-k)
\end{align*}
$$

- 似然$\Pbb(X | \theta)$是二项式分布，先验也{==凑==}$\theta^\spadesuit (1-\theta)^\heartsuit$的形式，{==选==}贝塔分布
- 若与似然相乘后，后验与先验属同一分布族，仅参数有变化，则该先验称为该似然的{==共轭先验==} (conjugate prior)
- 贝塔分布是二项式分布的共轭先验
- 先验分布的参数$(\alpha,\beta)$由观测者自己选，可视为观测者的领域知识；也可视为{==伪数据==}：在$X$前还观测过$\alpha+\beta$次抛掷，其中$\alpha$次正面

<!-- slide data-notes="" -->

##### 应用到机器学习

---

$$
\begin{align*}
    \quad \underbrace{\Pbb(\Theta|X)}_{\text{后验}} & = \frac{\overbrace{\Pbb(X|\Theta)}^{\text{似然}} \overbrace{\Pbb(\Theta)}^{\text{先验}}}{\underbrace{\Pbb(X)}_{\text{证据}}} = \Pbb(X|\Theta) \Pbb(\Theta) \Big/ \int \Pbb(X|\Theta) \Pbb(\Theta) \diff \Theta
\end{align*}
$$

- 假设有一个以$\Theta$为参数的 (生成式) 模型
- 先验：观测者对该模型为真实模型的初始信念
- 似然：在该模型为真实模型的条件下，观测到数据$X$的可能性
- 后验：在得到观测数据$X$后，对模型信念的修正

<div class="top2"></div>

根据是否利用先验，有两种估计$\Theta$的方式：

- 极大似然 (<u>m</u>aximum <u>l</u>ikelihood, ML)，$\Theta^{\text{ML}} = \mathop{\arg \max}_\Theta \Pbb(X|\Theta)$
- 最大后验 (<u>m</u>aximum <u>a</u> <u>p</u>osterior, MAP)，$\Theta^{\text{MAP}} = \mathop{\arg \max}_\Theta \Pbb(\Theta|X)$

<div class="top2"></div>

我的批注 前者为频率主义者的做法，后者为贝叶斯主义者的做法

<!-- slide vertical=true data-notes="" -->

##### 最大后验 全贝叶斯

---

以抛硬币为例，后验为

$$
\begin{align*}
    \quad \Pbb(\theta|X) = \frac{\theta^{\alpha + k - 1} (1-\theta)^{\beta + t - k - 1}}{\BetaFunc(\alpha+k,\beta+t-k)} = \BetaDist(\theta|\alpha+k,\beta+t-k)
\end{align*}
$$

<div class="top-2"></div>

若目标就是估计$\theta$，采用 MAP 估计：$\theta^{\text{MAP}} = \mathrm{\arg \max}_{\theta} ~ \Pbb(\theta|X)$

- 由于先验 (伪数据) 的存在，不会出现频率主义中$\theta^{\text{ML}} = 1$的情况

<div class="top2"></div>

若目标是对未知样本$\xhat$做预测，有两种做法：

- 先估计$\theta^{\text{MAP}}$再计算$\Pbb(\xhat|\theta^{\text{MAP}})$，这样做忽略了$\theta$的随机性，尤其当后验$\Pbb(\theta|X)$是个{==较均匀==}的分布时，只取一个点来做决策风险很大
- {==全贝叶斯==} (fully Bayesian)：考虑所有的$\theta$，根据后验做加权平均

<div class="top2"></div>

$$
\begin{align*}
    \quad \Pbb(\xhat|X) = \int \Pbb(\xhat|\theta) \Pbb(\theta|X) \diff \theta
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 小结 频率 <span style="font-weight:900">_vs._</span> 贝叶斯

---

在机器学习中的区别：是否考虑先验

当观测数据量很大时，先验 (伪数据) 就无足轻重了，两种做法不会有太大差别

当观测数据量不大时，先验对模型性能有显著影响 (归纳偏好)

- 先验是主观的，纯人为选取，没有标准
- 抛硬币问题选贝塔分布做先验就是图计算方便
- 利用共轭先验可以不用显式地积分求$\Pbb(X)$，肉眼就能看出结果

<div class="top2"></div>

先验需有适当的自由度，能通过调整参数灵活表示领域知识

<!-- slide data-notes="" -->

##### 再看朴素贝叶斯

---

朴素贝叶斯通过极大似然估计$\Pbb(y), ~ \Pbb(x_1 | y), ~ \ldots, ~ \Pbb(x_d | y)$

<div class="top-2"></div>

记$\alpha_k = \Pbb(y = k)$，于是$\sum_{k \in [c]} \alpha_k = 1$且

$$
\begin{align*}
    \quad \Pbb(y | \alpha_k) = \prod_{k \in [c]} \Pbb(y = k)^{\Ibb(y=k)} = \prod_{k \in [c]} \alpha_k^{\Ibb(y=k)}
\end{align*}
$$

<div class="top-5"></div>

是{==分类分布==}，伯努利分布的多元扩展，$c=2$即为伯努利分布

<div class="top1"></div>

伯努利分布呈$\theta^\spadesuit (1-\theta)^\heartsuit$的形式，共轭先验是贝塔分布

<div class="top1"></div>

$$
\begin{align*}
    \quad \BetaDist(\theta|\alpha,\beta) = \frac{\theta^{\alpha - 1} (1-\theta)^{\beta - 1}}{\int_0^1 \theta^{\alpha - 1} (1-\theta)^{\beta - 1} \diff \theta} = \frac{\theta^{\alpha - 1} (1-\theta)^{\beta - 1}}{\BetaFunc(\alpha,\beta)}
\end{align*}
$$

我的批注 分类分布的共轭先验是贝塔分布的多元扩展？

<!-- slide vertical=true data-notes="" -->

##### 狄利克雷分布

---

伽玛函数 (第二类欧拉积分) 和贝塔函数 (第一类欧拉积分)：

$$
\begin{align*}
    \quad \Gamma(m) & = \int_0^\infty \theta^{m - 1} \exp(- \theta) \diff \theta \\
    \BetaFunc(\alpha,\beta) & = \int_0^1 \theta^{\alpha - 1} (1-\theta)^{\beta - 1} \diff \theta = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+\beta)}
\end{align*}
$$

<div class="top-2"></div>

由贝塔函数可导出贝塔分布

$$
\begin{align*}
    \quad \BetaDist(\theta|\alpha,\beta) = \frac{\theta^{\alpha - 1} (1-\theta)^{\beta - 1}}{\BetaFunc(\alpha,\beta)} = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1-\theta)^{\beta - 1}
\end{align*}
$$

<div class="top-2"></div>

贝塔分布的多元扩展为狄利克雷分布

$$
\begin{align*}
    \quad \Dir(\alphav | \mv) = \frac{\Gamma(m_1 + \cdots + m_c)}{\Gamma(m_1) \cdots \Gamma(m_c)} \prod_{k \in [c]} \alpha_k^{m_k - 1}, \quad \sum_{k \in [c]} \alpha_k = 1
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 狄利克雷分布先验

---

@import "../dot/conjugate-prior.dot" {.left10per}

<div class="top1"></div>

记$\alpha_k = \Pbb(y = k)$，于是

$$
\begin{align*}
    \quad \Pbb(y | \alphav) = \prod_{k \in [c]} \Pbb(y = k)^{\Ibb(y=k)} = \prod_{k \in [c]} \alpha_k^{\Ibb(y=k)}, \quad \sum_{k \in [c]} \alpha_k = 1
\end{align*}
$$

<div class="top-2"></div>

设$\alphav$服从参数为$\mv$的狄利克雷分布：

$$
\begin{align*}
    \quad \Pbb(\alphav) = \Dir(\alphav | \mv) = \frac{\Gamma(m_1 + \cdots + m_c)}{\Gamma(m_1) \cdots \Gamma(m_c)} \prod_{k \in [c]} \alpha_k^{m_k - 1}, \quad \sum_{k \in [c]} \alpha_k = 1
\end{align*}
$$

<!-- slide data-notes="" -->

##### 狄利克雷分布后验

---

根据贝叶斯公式，后验

$$
\begin{align*}
    \quad \Pbb(\alphav | \yv) & \propto \Pbb(\alphav) \Pbb(\yv|\alphav) \\
    & = \left( \frac{\Gamma(m_1 + \cdots + m_c)}{\Gamma(m_1) \cdots \Gamma(m_c)} \prod_{k \in [c]} \alpha_k^{m_k - 1} \right) \left( \prod_{i \in [m]} \prod_{k \in [c]} \alpha_k^{\Ibb(y^{(i)}=k)} \right) \\
    & = \frac{\Gamma(m_1 + \cdots + m_c)}{\Gamma(m_1) \cdots \Gamma(m_c)} \prod_{k \in [c]} \alpha_k^{m_k - 1} \alpha_k^{\sum_{i \in [m]} \Ibb(y^{(i)}=k)}                                                  \\
    & = \frac{\Gamma(m_1 + \cdots + m_c)}{\Gamma(m_1) \cdots \Gamma(m_c)} \prod_{k \in [c]} \alpha_k^{A_k + m_k - 1}                                                                                        \\
    & \propto \Dir(\alphav | A_1 + m_1, \ldots, A_c + m_c)
\end{align*}
$$

<div class="top-3"></div>

其中$A_k = \sum_{i \in [m]} \Ibb(y^{(i)} = k)$为第$k$类样本数

这就验证了狄利克雷分布是分类分布的共轭先验

<!-- slide vertical=true data-notes="" -->

##### 最大后验估计

---

记$A_k = \sum_{i \in [m]} \Ibb(y^{(i)} = k)$为第$k$类样本数，后验

$$
\begin{align*}
    \quad \Pbb(\alphav | \yv) \propto \frac{\Gamma(m_1 + \cdots + m_c)}{\Gamma(m_1) \cdots \Gamma(m_c)} \prod_{k \in [c]} \alpha_k^{A_k + m_k - 1}
\end{align*}
$$

<div class="top-2"></div>

最大后验估计$\alpha_k$只需求解优化问题

$$
\begin{align*}
    \quad & \max_{\alpha_k} ~ \sum_{k \in [c]} (A_k + m_k - 1) \ln \alpha_k, \quad \st ~ \sum_{k \in [c]} \alpha_k = 1 \\[4pt]
    & \alpha_k^{\text{MAP}} = \frac{A_k + m_k - 1}{\lambda} = \frac{A_k + m_k - 1}{\lambda \sum_{j \in [c]} \alpha_j} = \frac{A_k + m_k - 1}{\sum_{j \in [c]} (A_j + m_j - 1)}
\end{align*}
$$

- 取$\mv = \onev$，则$\alpha_k^{\text{MAP}} = \alpha_k^{\text{ML}}$，此时狄利克雷分布退化为均匀分布，先验不包含观测者的任何偏好，最大后验估计退化为极大似然估计
- 取$\mv = \boldsymbol{2}$得到拉普拉斯平滑，此时的朴素贝叶斯才是真·贝叶斯

<!-- slide vertical=true data-notes="" -->

##### 真·朴素贝叶斯 小结

---

<div class="threelines column1-border-right-solid">

|  类别  |                         似然                         |                      共轭先验                       |                                     后验                                     |
| :----: | :--------------------------------------------------: | :-------------------------------------------------: | :--------------------------------------------------------------------------: |
| 枚举型 | $\left. \mathrm{Cate}(\yv \right\arrowvert \alphav)$ | $\left. \mathrm{Dir}(\alphav \right\arrowvert \mv)$ | $\left. \mathrm{Dir}(\alphav \right\arrowvert m_1 + A_1, \ldots, m_c + A_c)$ |

</div>

<div class="threelines column1-border-right-solid">

|    特征     |                              似然                               |                       共轭先验                       |                                     后验                                     |
| :---------: | :-------------------------------------------------------------: | :--------------------------------------------------: | :--------------------------------------------------------------------------: |
|   枚举型    |      $\left. \mathrm{Cate}(\xv \right\arrowvert \thetav)$       | $\left. \mathrm{Dir}(\thetav \right\arrowvert \mv)$  | $\left. \mathrm{Dir}(\thetav \right\arrowvert m_1 + A_1, \ldots, m_c + A_c)$ |
| $\{ 0,1 \}$ |    $\left. \mathrm{Bern}(x_j \right\arrowvert \theta_{kj})$     | $\left. \BetaDist(\theta_{kj} \right\arrowvert m,n)$ |  $\left. \BetaDist(\theta_{kj} \right\arrowvert m + B_{kj},n+\bar{B}_{kj})$  |
|   $\Nbb$    |      $\left. \mathrm{Mult}(\xv \right\arrowvert \thetav)$       | $\left. \mathrm{Dir}(\thetav \right\arrowvert \mv)$  | $\left. \mathrm{Dir}(\thetav \right\arrowvert m_1 + A_1, \ldots, m_c + A_c)$ |
|   $\Rbb$    | $\left. \Ncal(x_{kj} \right\arrowvert \mu_{kj}, \sigma_{kj}^2)$ |          均值不固定、精度固定时，为高斯分布          |
|      -      |                                -                                |         均值固定、精度不固定时，为威沙特分布         |
|      -      |                                -                                |       均值、精度都不固定时，为高斯-威沙特分布        |

</div>

我的批注 共轭先验的参数就是拉普拉斯平滑中的系数

<!-- slide data-notes="" -->

##### 再看线性回归

---

特征空间$\Xcal \subset \Rbb^d$，标记空间$\Ycal$，$\Xcal \times \Ycal$上的{==未知==}概率分布$\Dcal$

<div class="top-2"></div>

训练数据集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，其中$(\xv_i, y_i) \overset{\mathrm{iid}}{\sim} \Dcal$

假设数据的生成方式 (归纳偏好) 为

- 先在特征空间$\Xcal$中随机选取$\xv_i$
- 计算$y_i = \wv^\top \xv_i + \epsilon_i$，其中$\epsilon_i \sim \Ncal(0,\beta^{-1})$

<div class="top2"></div>

待估计参数是$\wv$和$\beta$，似然为

$$
\begin{align*}
    \quad \Pbb (D | \wv, \beta) & = \prod_{i \in [m]} \Pbb (\xv_i, y_i | \wv, \beta) = \prod_{i \in [m]} \Pbb (\xv_i) \Pbb (y_i | \xv_i, \wv, \beta) \\
    & \propto \prod_{i \in [m]} \Pbb (y_i | \xv_i, \wv, \beta) = \prod_{i \in [m]} \Ncal(\wv^\top \xv_i,\beta^{-1})
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 线性回归 极大似然

---

对数似然为

$$
\begin{align*}
    \quad \ln \Pbb (D | \wv, \beta) & = \const + \sum_{i \in [m]} \ln \Ncal(\wv^\top \xv_i,\beta^{-1}) \\
    & = \const + \frac{\ln \beta}{2} - \frac{\beta}{2} \sum_{i \in [m]}  (y_i - \wv^\top \xv_i)^2
\end{align*}
$$

极大似然估计$\wv$对应的优化问题为

$$
\begin{align*}
    \quad \min_{\wv} ~ \frac{\beta}{2} \sum_{i \in [m]} (y_i - \wv^\top \xv_i)^2 = \frac{\beta}{2} \| \Xv \wv - \yv \|_2^2
\end{align*}
$$

我的启示 在上页的假设下，{==极大似然估计等价于最小二乘回归==}

<!-- slide data-notes="" -->

##### 线性回归 贝叶斯视角

---

取先验分布$\Pbb(\wv) = \Ncal(\wv | \muv_0, \Sigmav_0)$

根据贝叶斯公式，后验为

<div class="top1"></div>

$$
\begin{align*}
    \quad \Pbb & (\wv | D) \propto \Pbb(\wv) \Pbb(D | \wv) \\
    & \propto \exp \left( -\frac{1}{2} (\wv - \muv_0)^\top \Sigmav_0^{-1} (\wv - \muv_0) \right) \prod_{i \in [m]} \exp \left( - \frac{\beta}{2} (y_i - \wv^\top \xv_i)^2 \right) \\
    & = \exp \left( -\frac{1}{2} (\wv - \muv_0)^\top \Sigmav_0^{-1} (\wv - \muv_0) - \frac{\beta}{2} \| \Xv \wv - \yv \|_2^2 \right) \\
    & \propto \exp \bigg( -\frac{1}{2} \wv^\top (\underbrace{\Sigmav_0^{-1} + \beta \Xv^\top \Xv}_{\Sigmav_m^{-1}}) \wv + \wv^\top (\underbrace{\Sigmav_0^{-1} \muv_0 + \beta \Xv^\top \yv}_{\Sigmav_m^{-1} \muv_m} )\bigg) \\
    & \propto \exp \left( -\frac{1}{2} (\wv - \muv_m)^\top \Sigmav_m^{-1} (\wv - \muv_m) \right) \propto \Ncal(\wv | \muv_m, \Sigmav_m)
\end{align*}
$$

<!-- slide vertical=true data-notes="" -->

##### 岭回归

---

特别的，取$\muv_0 = \zerov$、$\Sigmav_0 = \alpha^{-1} \Iv$，则

$$
\begin{align*}
    \quad \Pbb(\wv | D) & \propto \exp \left( -\frac{1}{2} (\wv - \muv_0)^\top \Sigmav_0^{-1} (\wv - \muv_0) - \frac{\beta}{2} \| \Xv \wv - \yv \|_2^2 \right) \\
    & = \exp \left( -\frac{\alpha}{2} \wv^\top \wv - \frac{\beta}{2} \| \Xv \wv - \yv \|_2^2 \right)
\end{align*}
$$

最大后验估计$\wv$只需求解优化问题

<div class="top1"></div>

$$
\begin{align*}
    \quad \min_{\wv} ~ \bigg\{ \underbrace{\frac{\beta}{2} \| \Xv \wv - \yv \|_2^2}_{\text{来自极大似然}~~~} + \underbrace{\frac{\alpha}{2} \|\wv\|_2^2}_{\text{来自先验}~~} \bigg\}
\end{align*}
$$

- 在原本最小二乘的基础上，多了一个关于$\wv$的$\ell_2$范数项
- 平方损失 + $\ell_2$范数 = {==岭回归==} (ridge regression)

<!-- slide vertical=true data-notes="" -->

##### <span style="font-weight:900">LASSO</span>

---

取先验分布$\Pbb(\wv) = \mathrm{Lap}(\wv | \muv_0, \alpha^{-1}) = \frac{\alpha}{2} \exp (- \alpha \| \wv - \muv_0 \|_1)$

根据贝叶斯公式

$$
\begin{align*}
    \quad \Pbb (\wv | D) \propto \Pbb(\wv) \Pbb(D | \wv) = \exp \left( - \alpha \| \wv - \muv_0 \|_1 - \frac{\beta}{2} \| \Xv \wv - \yv \|_2^2 \right)
\end{align*}
$$

<div class="top-3"></div>

特别的，取$\muv_0 = \zerov$，最大后验估计$\wv$只需求解优化问题

<div class="top1"></div>

$$
\begin{align*}
    \quad \min_{\wv} ~ \bigg\{ \underbrace{\frac{\beta}{2} \| \Xv \wv - \yv \|_2^2}_{\text{来自极大似然}~~~} + \underbrace{\alpha \| \wv \|_1}_{\text{来自先验}~~} \bigg\}
\end{align*}
$$

- 在原本最小二乘的基础上，多了一个关于$\wv$的$\ell_1$范数项
- 平方损失 + $\ell_1$范数 = 最小绝对值收敛和选择算子 (<u>l</u>east <u>a</u>bsolute <u>s</u>hrinkage and <u>s</u>election <u>o</u>perator, {==LASSO==})

<!-- slide vertical=true data-notes="" -->

##### 各种回归

---

<div class="threelines column1-border-right-solid">

|  模型  |               线性回归               |                             岭回归                             |                                                  LASSO                                                  |
| :----: | :----------------------------------: | :------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: |
|  数据  |                  >                   |                               >                                | 先随机选取$\xv_i$，再计算$y_i = \wv^\top \xv_i + \epsilon_i$，其中$\epsilon_i \sim \Ncal(0,\beta^{-1})$ |
|  先验  |                  -                   |                            高斯分布                            |                                              拉普拉斯分布                                               |
|  估计  |               极大似然               |                            最大后验                            |                                                最大后验                                                 |
|  优化  | $\min_{\wv} \| \Xv \wv - \yv \|_2^2$ | $\min_{\wv} \| \Xv \wv - \yv \|_2^2 + \lambda \|\wv\|_2^2 $ |                      $\min_{\wv} \| \Xv \wv - \yv \|_2^2 + \lambda \|\wv\|_1 $                       |
|  形式  |               平方损失               |                    平方损失 + $\ell_2$范数                     |                                         平方损失 + $\ell_1$范数                                         |
| 解析解 | $\wv = (\Xv^\top \Xv)^{-1} \Xv \yv$  |       $\wv = (\Xv^\top \Xv + \lambda \Iv)^{-1} \Xv \yv$        |                                                    -                                                    |

</div>

<!-- slide data-notes="" -->

##### 再看对率回归

---

设训练集$D = \{ (\xv_i, y_i) \}_{i \in [m]}$，似然为

$$
\begin{align*}
    \quad \Pbb(D | \wv) \propto \prod_{i \in [m]} \Pbb(y_i | \xv_i, \wv) = \prod_{i \in [m]} \sigma(y_i \wv^\top \xv_i) = \prod_{i \in [m]} \frac{1}{1 + \exp(- y_i \wv^\top \xv_i)}
\end{align*}
$$

取先验分布$\Pbb(\wv) = \Ncal(\wv | \zerov, \alpha^{-1} \Iv)$，则

$$
\begin{align*}
    \quad \Pbb(\wv | D) & \propto \exp \left( -\frac{\alpha}{2} \wv^\top \wv \right) \prod_{i \in [m]} \frac{1}{1 + \exp(- y_i \wv^\top \xv_i)}
\end{align*}
$$

最大后验估计$\wv$只需求解优化问题

$$
\begin{align*}
    \quad \min_{\wv} \sum_{i \in [m]} \ln (1 + \exp(- y_i \wv^\top \xv_i)) + \frac{\alpha}{2} \| \wv \|_2^2
\end{align*}
$$
